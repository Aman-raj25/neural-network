## **Neural Network from Scratch**  

This project implements a **Neural Network from scratch** using **NumPy**.  
It is a fully connected **feedforward neural network** trained using **backpropagation and gradient descent**.


## **ðŸ“Œ Features**  

âœ… Implements a **fully connected feedforward neural network**  
âœ… Supports **multiple hidden layers**  
âœ… Uses **Sigmoid, ReLU, and Softmax** activation functions  
âœ… **Backpropagation** for efficient weight updates  
âœ… **Cross-Entropy and Mean Squared Error (MSE) loss** functions  
âœ… **Mini-batch Gradient Descent** implementation  
âœ… **Evaluation on real datasets**  


## **Installation**  

### **Prerequisites**  
Ensure you have **Python 3.x** installed along with the required libraries:  

```bash
pip install numpy matplotlib scikit-learn
```


## **Usage**  

### **Clone the repository:**  
```bash
git clone https://github.com/Aman-raj25/neural-network.git
cd neural-network
```

### **Run the program:**  
```bash
python neural_network.ipynb
```


## **Expected Output**  

- The console will display **loss values** at each epoch.  
- A **visualization of the loss curve** will be generated.  
- **Predictions** on test data will be displayed.  


## **File Structure**  
```
â”œâ”€â”€ neural_network.py    # Main script for training & evaluation
â”œâ”€â”€ README.md            # Project documentation
```


## **Example Output**  
```python
Epoch 100: Loss = 0.245
Epoch 200: Loss = 0.138
...
Final Model Trained. Accuracy = 92.5%
```


## **Author**  
**Aman Raj**  


## **Contributing**  
Feel free to **fork this repository** and submit **pull requests**.  
For major changes, please **open an issue** first to discuss your ideas.  


### **Happy Coding! ðŸš€ðŸ”¥**

